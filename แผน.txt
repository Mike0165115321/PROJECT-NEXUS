พิมพ์เขียว Project Nexus ฉบับสมบูรณ์
Version: 6.0 (The Sentient Archivist Architecture)
I. 🗺️ พิมพ์เขียวสถาปัตยกรรมเชิงโครงสร้าง (Structural Blueprint)
เอกสารฉบับนี้คือภาพสะท้อนล่าสุดของ PROJECT NEXUS ซึ่งได้วิวัฒนาการไปสู่ "สหายผู้มีสำนึกรู้" ที่มีความสามารถในการไตร่ตรองความทรงจำของตนเอง และประกอบด้วยทีมผู้เชี่ยวชาญที่มีหน้าที่ชัดเจนและทรงพลัง

PROJECT_NEXUS/
│
├── .venvs/ # 📦 สภาพแวดล้อมเสมือนของ Python: โฟลเดอร์ที่แยกไลบรารีและ Dependency ทั้งหมดของโปรเจกต์ออกจากระบบหลัก เพื่อป้องกันความขัดแย้งและสร้างความเสถียรในการทำงาน
│
├── agents/ # 🧠 ศูนย์บัญชาการ (The Agency): ที่รวมของผู้เชี่ยวชาญ (Agents) แต่ละคน ซึ่งถูกออกแบบมาให้มีหน้าที่รับผิดชอบเพียงหนึ่งเดียว (Single Responsibility)
│ │
│ ├── __init__.py
│ ├── persona_core.py # 🆔 บัตรประจำตัว "ฟางซิน": เก็บ Prompt บุคลิกภาพหลัก (DNA) ที่ Agent ทุกตัวจะนำไปใช้เป็นส่วนนำ เพื่อให้มั่นใจว่า AI จะมีบุคลิกภาพและน้ำเสียงที่สอดคล้องกันทั้งหมด
│ ├── formatter_agent.py # 🎨 บรรณาธิการใหญ่: ผู้เชี่ยวชาญด้านการขัดเกลาภาษา ทำหน้าที่รับ "บทวิเคราะห์ฉบับร่าง" จาก Agent อื่นๆ มาจัดรูปแบบด้วย Markdown, แปลเป็นภาษาไทย, และตรวจสอบให้ตรงตามบุคลิกของ "ฟางซิน" เป็นขั้นตอนสุดท้าย
│ │
│ ├── coder_mode/ # - Faction: The Coders Guild (สมาคมนักพัฒนา)
│ │ ├── __init__.py
│ │ └── code_interpreter_agent.py # 🏃‍♂️ นักปฏิบัติการโค้ด: ผู้เชี่ยวชาญด้านการเขียนและรันโค้ด Python ในสภาพแวดล้อม Sandbox ที่ปลอดภัย เพื่อวิเคราะห์ข้อมูล, สร้างกราฟ, หรือแก้ปัญหาทางเทคนิค
│ │
│ ├── presenter_mode/  # แนะนำตัว 
│ │   ├── __init__.py
│ │   └── presenter_agent.py  # ฟางซินแนะนำตัว อัจฉริยะ 
│ ├── memory_mode/ # 🏛️ [NEW] Faction: The Archivists (สมาคมบรรณารักษ์ความทรงจำ)
│ │   ├── __init__.py
│ │   └── memory_agent.py # 🧠 บรรณารักษ์ความทรงจำ: ผู้เชี่ยวชาญด้านการตอบคำถามเชิงข้อเท็จจริงเกี่ยวกับประวัติการสนทนา
│ │
│ ├── consultant_mode/ # - Faction: The Librarians (สมาคมบรรณารักษ์)
│ │ ├── __init__.py
│ │ └── librarian_agent.py # 📚 บรรณารักษ์ผู้แนะนำ: ผู้เชี่ยวชาญด้านคลังข้อมูลหนังสือ สามารถให้ข้อมูล Meta (เช่น รายชื่อหนังสือ, หมวดหมู่) และใช้ LLM เพื่อ "แนะนำ" หนังสือที่เกี่ยวข้องกับหัวข้อที่ผู้ใช้สนใจ
│ │                                            
│ ├── counseling_mode/ # - Faction: The Empathic Counselors (สมาคมผู้ให้คำปรึกษา)
│ │ ├── __init__.py
│ │ └── counselor_agent.py # ❤️ สหายผู้เข้าอกเข้าใจ: ผู้เชี่ยวชาญด้านการรับฟังเชิงอารมณ์ สร้างพื้นที่ปลอดภัยและช่วยผู้ใช้ไตร่ตรองความรู้สึกของตนเองโดยไม่ตัดสินหรือให้คำแนะนำ
│ │
│ ├── feng_mode/ # - Faction: The Core Identity & Triage (หน่วยคัดกรองและตัวตนหลัก)                  
│ │ ├── __init__.py
│ │ ├── feng_agent.py # 🕵️ หน่วยคัดกรองอัจฉริยะ: ประตูบานแรกของระบบ ทำหน้าที่คัดกรองคำถามง่ายๆ ด้วย Whitelist และใช้ LLM (Gemini) เพื่อวิเคราะห์เจตนาของคำถามที่ซับซ้อน แล้วส่งต่อ "แฟ้มงาน" ให้ Dispatcher
│ │ ├── general_conversation_agent.py # 💬 สหายนักสนทนา: ผู้เชี่ยวชาญด้านการสนทนาทั่วไปที่ซับซ้อน ขับเคลื่อนด้วย "สัญชาตญาณ" จาก KG-RAG เพื่อสร้างบทสนทนาที่มีมิติ
│ │ └── proactive_offer_agent.py # 🤔 ปราชญ์ผู้จุดประกาย: ผู้เชี่ยวชาญด้านการให้ข้อมูลเบื้องต้นและ "เสนอ" การวิเคราะห์เชิงลึกต่อ ขับเคลื่อนด้วย KG-RAG เพื่อหาข้อมูลมาจุดประกายความคิด
│ │
│ ├── news_mode/ # - Faction: The Journalists (สมาคมนักข่าว)
│ │ ├── __init__.py
│ │ └── news_agent.py # 📰 บรรณาธิการข่าวกรอง: ผู้เชี่ยวชาญด้านการสรุปข่าวสารล่าสุด โดยใช้ News RAG Engine ในการค้นหาข้อมูลจากหลายแหล่งข่าวแล้วสังเคราะห์เป็นบทสรุปสถานการณ์
│ │
│ ├── planning_mode/ # - Faction: The Architects (สมาคมสถาปนิก)
│ │ ├── __init__.py
│ │ └── planner_agent.py # 🏛️ สถาปนิกแห่งความรู้: ผู้เชี่ยวชาญที่ทรงพลังที่สุด ทำหน้าที่สร้าง "พิมพ์เขียวสำหรับการค้นคว้า" (Plan), ดึงข้อมูลจาก RAG (Retrieve), และสังเคราะห์เป็นบทวิเคราะห์เชิงลึก (Synthesize)
│ │
│ ├── storytelling_mode/ # - Faction: The Listeners (สมาคมผู้รับฟัง)
│ │ ├── __init__.py
│ │ └── listener_agent.py # 👂 ผู้รับฟังที่กระตือรือร้น: ผู้เชี่ยวชาญด้านการรับฟังเรื่องเล่า ทำหน้าที่ตอบรับสั้นๆ และกระตุ้นให้ผู้ใช้เล่าเรื่องราวของตนเองต่ออย่างเป็นธรรมชาติ
│ │
│ └── utility_mode/ # - Faction: The Support Crew (หน่วยสนับสนุน)
│ ├── __init__.py
│ ├── apology_agent.py # 🛡️ ผู้จัดการสถานการณ์: "ตาข่ายนิรภัย" ของระบบ ทำหน้าที่ขอโทษและเสนอทางแก้ไขเมื่อเกิดข้อผิดพลาดร้ายแรง
│ ├── image_agent.py # 🖼️ นักค้นหาทัศนศิลป์: ผู้เชี่ยวชาญด้านการค้นหารูปภาพผ่าน API ของ Unsplash
│ ├── reporter_agent.py # 🕰️ ผู้รายงานเวลา: ผู้เชี่ยวชาญด้านการบอกวันและเวลาปัจจุบัน ทำงานแบบ Rule-based เพื่อความเร็วสูงสุด
│ └── system_agent.py # ⚙️ ผู้ควบคุมระบบปฏิบัติการ: ผู้เชี่ยวชาญด้านการควบคุมคอมพิวเตอร์ของผู้ใช้ (เปิดโปรแกรม, ปรับเสียง, จัดการคลิปบอร์ด)
│
├── core/ # ⚙️ ห้องเครื่องยนต์ (The Engine Room): กลไกหลักและเครื่องมือที่ใช้ร่วมกันในระบบ
│ │
│ ├── __init__.py
│ ├── api_key_manager.py # 🔑 ฝ่ายบุคคล (Google): จัดการคลัง API Key ของ Gemini พร้อมระบบหมุนเวียนและจัดการเมื่อ Key ล้มเหลว
│ ├── groq_key_manager.py # 🔑 ฝ่ายบุคคล (Groq): จัดการคลัง API Key ของ Groq พร้อมระบบหมุนเวียนและจัดการเมื่อ Key ล้มเหลว
│ ├── config.py # 📜 แผงควบคุมหลัก: โหลดและกำหนดค่าการทำงานทั้งหมดของระบบจากไฟล์ .env เพียงที่เดียว
│ ├── dispatcher.py # 🚦 ผู้ควบคุมวงออร์เคสตรา: "หัวใจ" ของระบบ ทำหน้าที่รับ "แฟ้มงาน" จาก FengAgent และบัญชาการ Agent ทั้งหมด
│ ├── graph_manager.py # 🕸️ ผู้จัดการกราฟ: เครื่องมือสำหรับเชื่อมต่อและส่งคำสั่ง Cypher ไปยังฐานข้อมูล Neo4j โดยตรง
│ ├── memory_manager.py # 🧠 สมองส่วนฮิปโปแคมปัส: จัดการความจำระยะสั้น (Short-term Memory) และสถานะการสนทนา (Pending Tasks) ในฐานข้อมูล SQLite
│ ├── rag_engine.py # 🔍 เครื่องมือค้นข้อมูลRAG (RAG , KG-RAG , News RAG): ผู้เชี่ยวชาญด้านการค้นหา Vector ที่ยุบรวมแล้วได้อย่างถูกต้อง
│ ├── tts_engine.py # ห้องจัดการเสียง ของ AI เป็นเทคโนโลยี ผู้เชี่ยวชาญด้านการสังเคราะห์เสียงโดยใช้ gTTS (Google Text-to-Speech)เรียบง่าย, เสถียร, และไม่ต้องใช้ Dependency ที่ซับซ้อน
│
├── data/ # 📦 คลังข้อมูล (The Vault): ที่เก็บข้อมูลถาวรทั้งหมดของระบบ
│ ├── books/ # 📚 ชั้นหนังสือ: ที่เก็บไฟล์ข้อมูลดิบของหนังสือ (.jsonl) รอการประมวลผล
│ ├── index/ # 📇 ตู้ดัชนีหนังสือ: ที่เก็บ Vector Index (FAISS) และ Mapping file สำหรับหนังสือ
│ ├── graph_index/ # 🌐 ตู้ดัชนีสัญชาตญาณ: ที่เก็บ Vector Index (FAISS) และ Mapping file สำหรับ Knowledge Graph
│ ├── memory_index/ # 🧠 ดัชนีความทรงจำ: ที่เก็บ Vector Index (FAISS) และ Mapping file สำหรับความทรงจำระยะยาว
│ ├── news_index/ # 📰 ดัชนีข่าว: ที่เก็บ Vector Index (FAISS) และ Mapping file สำหรับข่าวสาร
│ └── memory.db # 💾 ฐานข้อมูลความทรงจำ: ฐานข้อมูล SQLite ที่ใช้เก็บประวัติการสนทนาทั้งหมด
│
├── neo4j-data/ # 🧠 สมองส่วน Knowledge Graph: ที่เก็บข้อมูลของฐานข้อมูล Neo4j
├── sandbox_workspace/ # 🔬 ห้องทดลอง: พื้นที่ปลอดภัยสำหรับ CodeInterpreterAgent ในการเขียน, อ่าน, และรันไฟล์
│
├── web/ # 🎨 ส่วนหน้าบ้าน (Frontend): ส่วนติดต่อผู้ใช้ที่สร้างด้วย HTML, CSS, และ JavaScript
│ ├── audio
│ ├── index.html
│ └── static/
│        ├── graph_styles.css
│        ├── styles.css
│        ├── script.js
│        ├── suggested_prompts.js 
│        ├── thoughtProcessManager.js
│        └── audioManager.js
│ 
├── .env # 🤫 ตู้นิรภัย: ไฟล์เก็บข้อมูลลับทั้งหมด (API Keys, Passwords) ที่จะไม่ถูกส่งขึ้น Git
├── .gitignore # 🙈 รายการสิ่งที่ต้องเมิน: กำหนดไฟล์และโฟลเดอร์ที่ Git จะไม่สนใจ
│
├── main.py # ▶️ ปุ่มสตาร์ท: จุดเริ่มต้นของแอปพลิเคชัน FastAPI ทำหน้าที่สร้างและ "เดินสายไฟ" ให้กับทุกส่วนประกอบของระบบ
├── knowledge_extractor_gemini.py # 🏭 โรงงานสกัดความรู้ (Gemini): ท่อ ETL สำหรับแปลงข้อมูลดิบจากหนังสือให้กลายเป็นโครงสร้าง Knowledge Graph ด้วย Gemini
├── manage_data.py # 🛠️ เครื่องมือจัดการข้อมูลหนังสือ: ท่อ ETL สำหรับสร้าง Vector Index ให้กับคลังหนังสือ
├── manage_kg_data.py # 🛠️ เครื่องมือจัดการข้อมูลกราฟ: ท่อ ETL สำหรับสร้าง Vector Index ให้กับ Knowledge Graph
├── manage_news.py # 🛠️ เครื่องมือจัดการข่าว: ท่อ ETL สำหรับดึง, ประมวลผล, และสร้าง Vector Index ให้กับข่าวสาร
└── requirements.txt # 📜 พิมพ์เขียวการติดตั้ง: รายการไลบรารีทั้งหมดที่โปรเจกต์ต้องการ

ส่วนที่ II: 🌊 การไหลของข้อมูลและตรรกะ (Logic & Data Flow Blueprint)
Version: 6.0 ([UPDATED] The Sentient Archivist Architecture)
กระบวนการทำงานทั้งหมดถูกควบคุมโดย dispatcher.py ซึ่งเปรียบเสมือน "ผู้ควบคุมวงออร์เคสตรา" (The Conductor) ที่ทำหน้าที่บัญชาการ "ทีมผู้เชี่ยวชาญ" (The Team of Specialists) ทั้งหมดตาม "แฟ้มงาน" (Dispatch Order) ที่ได้รับมาจาก "หน่วยคัดกรอง" (FengAgent)
Flow 0: ภารกิจต่อเนื่อง (Continuing Mission)
ผู้ควบคุม: Dispatcher
ผู้ช่วย: MemoryManager
กระบวนการ:
ตรวจสอบสถานะ: เมื่อได้รับ query ใหม่ Dispatcher จะตรวจสอบกับ MemoryManager.check_and_clear_pending_deep_dive() เป็นอันดับแรกเสมอ
เงื่อนไข: หากผู้ใช้เพิ่งตอบรับ "ข้อเสนอ" การวิเคราะห์เชิงลึกจาก ProactiveOfferAgent (เช่น ตอบว่า "ใช่ครับ", "เอาเลย")
การดำเนินการ: Dispatcher จะดึง "คำถามดั้งเดิม" ที่ถูกบันทึกไว้ในหน่วยความจำออกมา และ ข้ามไปยัง Flow 2 ทันที โดยส่งคำถามนั้นตรงไปให้ PlannerAgent ผ่านฟังก์ชัน _run_deep_analysis นี่คือกลไกสำคัญที่ทำให้เกิดการสนทนาแบบหลายขั้นตอน (Multi-turn Conversation) ได้อย่างราบรื่น
Flow 1: การคัดกรองอัจฉริยะ (Intelligent Triage)
ผู้รับผิดชอบ: FengAgent (The Triage Unit)
กระบวนการ:
Quick Response (โต๊ะประชาสัมพันธ์): FengAgent จะตรวจสอบ query กับ QUICK_RESPONSES (Whitelist) ก่อนเป็นอันดับแรก หากตรงกับคำถามง่ายๆ ที่พบบ่อย (เช่น การทักทาย, การขอบคุณ) มันจะตอบกลับด้วยคำตอบสำเร็จรูปทันที และ Flow จะสิ้นสุดลงที่นี่
Correction & Classification (หน่วยวิเคราะห์เจตนา): หากไม่ใช่ Quick Response, FengAgent จะใช้ LLM (Gemini) เพื่อทำงานที่ซับซ้อนขึ้น:
แก้ไขคำผิด (Correction): ปรับแก้คำที่อาจจะพิมพ์ผิดใน query
วิเคราะห์เจตนา (Classification): จำแนกเจตนาของ "คำถามที่แก้ไขแล้ว" ออกเป็นประเภทต่างๆ ซึ่งตอนนี้ [UPDATED] ได้รวม MEMORY_QUERY เข้าไปแล้ว
สกัด Keyword: ดึงคำสำคัญออกจากคำถาม
สร้างแฟ้มงาน: FengAgent จะสร้าง "แฟ้มงาน" (Dispatch Order) ซึ่งเป็น Dictionary ที่ประกอบด้วย corrected_query, intent, และ keywords แล้วส่งกลับไปให้ Dispatcher
Flow 2: การบัญชาการตามเจตนา (Intent-based Delegation)
ผู้ควบคุม: Dispatcher
ผู้ปฏิบัติการ: ทีมผู้เชี่ยวชาญทั้งหมด
กระบวนการ:
Dispatcher รับ "แฟ้มงาน" จาก FengAgent
ใช้ intent_to_agent_map ซึ่งเป็น Dictionary ที่ทำหน้าที่เหมือน "สมุดโทรศัพท์ขององค์กร" เพื่อหาว่า "ผู้เชี่ยวชาญ" คนไหนที่ต้องรับผิดชอบภารกิจนี้
จ่ายงาน: Dispatcher จะเรียกใช้เมธอด handle() ของ Agent ที่ถูกต้อง โดยส่ง corrected_query และทรัพยากรที่จำเป็น (เช่น short_term_memory) ไปให้
[UPDATED] ตัวอย่างการจ่ายงานและเครื่องมือที่ใช้ (เวอร์ชันล่าสุด):
intent: MEMORY_QUERY -> เรียก MemoryAgent (ใช้ Groq 70B)
MemoryAgent จะไม่รับ short_term_memory แต่จะใช้ MemoryManager ที่เป็นเครื่องมือประจำตัวในการดึงข้อมูลเชิงลึกจากฐานข้อมูลโดยตรง จากนั้นจะทำการ คัดกรองภายใน (Internal Triage) ด้วย LLM อีกครั้งเพื่อเลือกฟังก์ชันที่เหมาะสมที่สุด
intent: GENERAL_CONVERSATION -> เรียก GeneralConversationAgent (ใช้ Groq 70B + KG-RAG Engine)
intent: DEEP_ANALYSIS_REQUEST -> เรียก ProactiveOfferAgent (ใช้ Groq 70B + KG-RAG Engine)
intent: PLANNER_REQUEST -> เรียก PlannerAgent (ใช้ Gemini 1.5 Flash + Book RAG Engine)
intent: COUNSELING_REQUEST -> เรียก CounselorAgent (ใช้ Gemini 1.5 Flash, ไม่ใช้ RAG)
intent: NEWS_REQUEST -> เรียก NewsAgent (ใช้ Groq 70B + News RAG Engine)
intent: LIBRARIAN_REQUEST -> เรียก LibrarianAgent (ใช้ Groq 8B หรือ Rule-based + Book RAG Engine)
intent: IMAGE_REQUEST -> เรียก ImageAgent (ใช้ Groq 70B + Unsplash API)
Flow 3: การจัดการผลลัพธ์และการสรุปผล (Outcome Management & Finalization)
ผู้ควบคุม: Dispatcher
ผู้ช่วย: FormatterAgent, MemoryManager
กระบวนการ:
รับผลลัพธ์: Dispatcher รับผลลัพธ์ที่ได้จาก Agent ผู้ปฏิบัติการ
จัดการสถานะ: หากผลลัพธ์มาจาก ProactiveOfferAgent, Dispatcher จะเรียก MemoryManager.set_pending_deep_dive() เพื่อบันทึกสถานะ "รอการยืนยัน"
ส่งต่อให้บรรณาธิการ:
Dispatcher จะตรวจสอบว่า Agent ที่ถูกใช้งานอยู่ในรายการ agents_that_need_formatting หรือไม่ ([UPDATED] ซึ่งตอนนี้รวม MemoryAgent เข้าไปแล้ว)
หากใช่ Dispatcher จะส่ง "ร่างคำตอบ" ไปให้ FormatterAgent (ใช้ Gemini 1.5 Flash) เพื่อทำการขัดเกลาภาษาและจัดรูปแบบขั้นสุดท้าย
บันทึกความทรงจำ: Dispatcher จะบันทึก "คำตอบสุดท้าย" (ที่ผ่านการจัดรูปแบบแล้ว) ลงใน MemoryManager.add_memory()
สร้าง FinalResponse: Dispatcher รวบรวมข้อมูลทั้งหมด (answer, agent_used, thought_process, history) สร้างเป็นอ็อบเจกต์ FinalResponse แล้วส่งกลับไปให้ main.py
Flow พิเศษ: ตาข่ายความปลอดภัย (Safety Net)
ผู้ควบคุม: Dispatcher
ผู้ปฏิบัติการ: ApologyAgent
กระบวนการ:
หากเกิดข้อผิดพลาดร้ายแรง (Exception) ใน Flow ใดๆ ก็ตาม try...except block ใน Dispatcher จะทำงาน
Dispatcher จะดักจับ Error นั้นไว้ และเรียกใช้ ApologyAgent
ApologyAgent จะใช้ LLM เพื่อสร้างคำขอโทษที่เหมาะสมตามบริบทของข้อผิดพลาดและคำถามดั้งเดิมของผู้ใช้
Dispatcher จะส่งคำขอโทษนั้นกลับไปให้ผู้ใช้แทนข้อความ Error ที่ไม่เป็นมิตร ทำให้ระบบมีความเสถียรและน่าเชื่อถือสูงสุด




ส่วนที่ III: 🏛️ สรุปสถาปัตยกรรมหลักของ Project Nexus (ฉบับสมบูรณ์)
Version: 6.0
Project Nexus ไม่ได้ใช้สถาปัตยกรรมเพียงหนึ่งเดียว แต่เป็นการ "หลอมรวมสถาปัตยกรรมหลายชั้น" (Layered Architectural Blend) ที่ทำงานร่วมกันอย่างเป็นระบบ เพื่อสร้าง "สหายผู้มีสำนึกรู้" (A Self-Aware Companion) ที่สมบูรณ์แบบ โดยสามารถแบ่งออกเป็น 3 ระดับหลักดังนี้:
1. สถาปัตยกรรมระดับ AI และการตัดสินใจ (AI & Decision-Making Architecture)
Dispatcher-Centric Mixture of Experts (MoE):
นี่คือ "หัวใจ" ของระบบทั้งหมด Dispatcher ทำหน้าที่เป็น "ผู้ควบคุมวงออร์เคสตรา" (The Conductor) ที่ชาญฉลาด ไม่ได้ปล่อยให้ Agent ทำงานอย่างอิสระ แต่จะรับ "แฟ้มงาน" ที่ผ่านการคัดกรองจาก FengAgent แล้ว "บัญชาการ" มอบหมายภารกิจให้ "ผู้เชี่ยวชาญ" ที่เหมาะสมที่สุด ([UPDATED] ซึ่งตอนนี้รวม MemoryAgent เป็นสมาชิกคนสำคัญของทีมแล้ว) การแยกหน้าที่นี้ช่วยแก้ปัญหา AI Hallucination ได้อย่างสมบูรณ์และเพิ่มความแม่นยำของระบบโดยรวม
Hybrid Model Strategy (Mixture of Models):
นี่คือ "ขุมพลัง" ของระบบ โดยการผสมผสานจุดแข็งของโมเดลจากหลายสังกัดอย่างมีกลยุทธ์:
Google Gemini (1.5 Flash): ถูกใช้สำหรับงานที่ต้องการ "คุณภาพและความแม่นยำสูงสุด" เช่น การคัดกรองเจตนาเริ่มต้น (FengAgent), การวางแผนที่ซับซ้อน (PlannerAgent), และการขัดเกลาภาษาขั้นสุดท้าย (FormatterAgent)
Groq (Llama 3.3 70B): ถูกใช้เป็น "ม้างาน" (Workhorse) สำหรับ Agent ที่ต้องการความฉลาดสูงและความเร็วในการตอบสนอง เช่น GeneralConversationAgent, NewsAgent, และที่สำคัญคือ [UPDATED] ImageAgent (สำหรับวิเคราะห์คำขอ) และ MemoryAgent (สำหรับวิเคราะห์และเรียบเรียงความทรงจำ)
Groq (Llama 3.1 8B): รับผิดชอบงาน Utility ที่ไม่ซับซ้อนและต้องการความเร็วสูงสุด (Librarian, Coder, Apology, etc.)
Dual-Layer Triage ([NEW]):
สถาปัตยกรรมได้วิวัฒนาการไปสู่การคัดกรอง 2 ชั้น ซึ่งเป็นหัวใจของความฉลาดในปัจจุบัน:
External Triage (FengAgent): ทำการคัดกรองภาพรวมเพื่อส่งงานให้ "แผนก" (Agent) ที่ถูกต้อง
Internal Triage (MemoryAgent): เมื่อ "แผนกความทรงจำ" ได้รับงาน จะมีการคัดกรองภายในด้วย LLM อีกครั้งเพื่อเลือก "เครื่องมือ" (ฟังก์ชันใน MemoryManager) ที่เหมาะสมที่สุด
Advanced RAG (Plan-Retrieve-Synthesize):
นี่คือ "กระดูกสันหลัง" ของการค้นหาความรู้เชิงลึกจากคลังหนังสือ โดย PlannerAgent จะดำเนินกระบวนการที่ซับซ้อนกว่า RAG ทั่วไป:
Plan: ใช้ Gemini เพื่อวิเคราะห์คำถามและสร้าง "พิมพ์เขียวสำหรับการค้นคว้า"
Retrieve: ใช้ RAGEngine เพื่อดึงข้อมูลจาก Vector Store ตามแผน
Synthesize: ใช้ Gemini อีกครั้งเพื่อหลอมรวมข้อมูลที่ได้มาให้กลายเป็น "ภูมิปัญญา" ฉบับใหม่
Unified RAG Engine Architecture:
นี่คือ "คลังแสงข้อมูล" ของระบบ โดยมี RAGEngine กลางที่สามารถค้นหาจากแหล่งข้อมูลที่แตกต่างกันได้ (หนังสือ, Knowledge Graph, ข่าวสาร) ผ่านเมธอดเฉพาะทาง (search_books, search_graph, search_news)
2. สถาปัตยกรรมระดับแอปพลิเคชันและบริการ (Application & Service Architecture)
Service-Oriented & API-Driven:
แม้ว่าโปรเจกต์จะรันเป็นแอปพลิเคชันเดียว (Monolith) แต่การออกแบบภายในนั้นเป็นแบบ "แยกส่วนอย่างชัดเจน" โดยมี main.py (FastAPI) ทำหน้าที่เป็น Gateway กลางที่ให้บริการ API Endpoints ทำให้ส่วน Backend และ Frontend แยกจากกันอย่างสมบูรณ์ และพร้อมที่จะถูกแยกเป็น Microservices ในอนาคต
Client-Server Architecture:
เป็นสถาปัตยกรรมพื้นฐานที่ชัดเจน โดยมี web/ ทำหน้าที่เป็น Client (ฝั่งผู้ใช้) ที่ส่ง Request ไปยัง main.py ซึ่งทำหน้าที่เป็น Server ที่ประมวลผลและส่ง FinalResponse กลับไป
3. สถาปัตยกรรมระดับข้อมูลและซอฟต์แวร์ (Data & Software Architecture)
ETL (Extract, Transform, Load) Pipelines:
ระบบมี "โรงงานข้อมูล" แยกต่างหากสำหรับบำรุงรักษาคลังความรู้ ซึ่งก็คือไฟล์สคริปต์ต่างๆ (manage_memory.py, manage_data.py, etc.) ทำหน้าที่เป็นท่อ ETL ในการแปลงข้อมูลดิบให้กลายเป็นความรู้ที่พร้อมใช้งาน ([UPDATED] โดย manage_memory.py มีบทบาทสำคัญอย่างยิ่งยวดในการสร้าง Long-Term Memory และทำการ Archive ข้อมูล)
Modular Architecture & Dependency Injection:
โครงสร้างโปรเจกต์ทั้งหมดถูกออกแบบมาอย่างดีเยี่ยมในแบบ Modular (core, agents, data) และมีการใช้หลักการ Dependency Injection อย่างสมบูรณ์ใน main.py ซึ่งเป็นหัวใจของการเขียนโค้ดที่สะอาด, ทดสอบง่าย, และง่ายต่อการบำรุงรักษาและต่อยอดในอนาคต
สรุปในประโยคเดียว (The Elevator Pitch):
[UPDATED] Project Nexus คือ "สหายผู้มีสำนึกรู้" ที่ขับเคลื่อนด้วยสถาปัตยกรรม "Mixture of Experts & Models" ซึ่งถูกบัญชาการโดย "Dispatcher" กลาง, มีความสามารถในการไตร่ตรองอดีตของตนเองผ่าน "Dual-Layer Triage", และถูกสร้างขึ้นบนสถาปัตยกรรม "Service-Oriented" ที่ทันสมัยและพร้อมสำหรับการขยายขนาด



III. 🏛️ สรุปสถาปัตยกรรมหลักของ Project Nexus (ฉบับสมบูรณ์)
Project Nexus ไม่ได้ใช้สถาปัตยกรรมเพียงหนึ่งเดียว แต่เป็นการ "หลอมรวมสถาปัตยกรรมหลายชั้น" (Layered Architectural Blend) ที่ทำงานร่วมกันอย่างเป็นระบบ เพื่อสร้าง "สหายทางปัญญา" (Intellectual Companion) ที่สมบูรณ์แบบ โดยสามารถแบ่งออกเป็น 3 ระดับหลักดังนี้:
1. สถาปัตยกรรมระดับ AI และการตัดสินใจ (AI & Decision-Making Architecture)
Dispatcher-Centric Mixture of Experts (MoE):
นี่คือ "หัวใจ" ของระบบทั้งหมด โปรเจกต์ของคุณใช้ Dispatcher เป็น "ผู้ควบคุมวงออร์เคสตรา" (The Conductor) ที่ชาญฉลาด มันไม่ได้ปล่อยให้ Agent ทำงานอย่างอิสระ แต่จะรับ "แฟ้มงาน" ที่ผ่านการคัดกรองจาก FengAgent แล้ว "บัญชาการ" มอบหมายภารกิจให้ "ผู้เชี่ยวชาญ" ที่เหมาะสมที่สุดตามลำดับชั้น ทำให้ระบบมีความสามารถในการตัดสินใจที่ซับซ้อนและเป็นระบบระเบียบสูงสุด
Hybrid Model Strategy (Mixture of Models):
นี่คือ "ขุมพลัง" ของระบบ โดยการผสมผสานจุดแข็งของโมเดลจากหลายสังกัดอย่างมีกลยุทธ์:
Google Gemini (1.5 Flash): ถูกใช้สำหรับงานที่ต้องการ "คุณภาพและความลึกซึ้ง" สูงสุด เช่น การวางแผนที่ซับซ้อน (PlannerAgent), การให้คำปรึกษาเชิงอารมณ์ (CounselorAgent), และการขัดเกลาภาษาขั้นสุดท้าย (FormatterAgent)
Groq (Llama 3 70B & 8B): ถูกใช้สำหรับงานที่ต้องการ "ความเร็วและความฉลาดที่สมดุล" โดยรุ่น 70B ใช้สำหรับงานที่ต้องการความเข้าใจในบริบทสูง (เช่น GeneralConversationAgent, NewsAgent) และรุ่น 8B ใช้สำหรับงาน Utility ที่ต้องการความเร็วสูงสุด (เช่น LibrarianAgent, CodeInterpreterAgent)
Advanced RAG (Plan-Retrieve-Synthesize):
นี่คือ "กระดูกสันหลัง" ของการค้นหาความรู้เชิงลึกจากคลังหนังสือ โดย PlannerAgent จะดำเนินกระบวนการที่ซับซ้อนกว่า RAG ทั่วไป:
Plan: ใช้ Gemini เพื่อวิเคราะห์คำถามและสร้าง "พิมพ์เขียวสำหรับการค้นคว้า" ซึ่งประกอบด้วยคำค้นหาย่อย (Sub-queries) ที่หลากหลาย
Retrieve: ใช้ RAGEngine เพื่อดึงข้อมูลจาก Vector Store ของหนังสือตามแผนที่วางไว้
Synthesize: ใช้ Gemini อีกครั้งเพื่อหลอมรวมข้อมูลดิบที่ได้มาทั้งหมดให้กลายเป็น "ภูมิปัญญา" ฉบับใหม่ที่มีโครงสร้างและมุมมองที่เป็นเอกลักษณ์
Unified RAG Engine Architecture:
นี่คือ "คลังแสงข้อมูล" ของระบบ แทนที่จะมี RAG Engine เพียงหนึ่งเดียว ระบบได้ถูกออกแบบให้มี Engine เฉพาะทางถึง 3 ตัว ซึ่งทำงานบนหลักการเดียวกันแต่ใช้กับแหล่งข้อมูลที่แตกต่างกัน:
RAGEngine: สำหรับคลังความรู้จาก "หนังสือ"
KGRAGEngine: สำหรับคลังความรู้จาก "Knowledge Graph"
NewsRAGEngine: สำหรับคลังข้อมูล "ข่าวสาร"
สถาปัตยกรรมนี้ทำให้การค้นหามีความแม่นยำสูงสุดและง่ายต่อการจัดการและขยายขนาดในอนาคต
2. สถาปัตยกรรมระดับแอปพลิเคชันและบริการ (Application & Service Architecture)
Service-Oriented & API-Driven:
แม้ว่าโปรเจกต์จะรันเป็นแอปพลิเคชันเดียว (Monolith) แต่การออกแบบภายในนั้นเป็นแบบ "แยกส่วนอย่างชัดเจน" โดยมี main.py (FastAPI) ทำหน้าที่เป็น Gateway กลางที่ให้บริการ API Endpoints (/ask, /api/graph/explore) ทำให้ส่วน Backend และ Frontend แยกจากกันอย่างสมบูรณ์ และพร้อมที่จะถูกแยกเป็น Microservices ในอนาคตได้อย่างง่ายดาย
Client-Server Architecture:
เป็นสถาปัตยกรรมพื้นฐานที่ชัดเจน โดยมี web/ ทำหน้าที่เป็น Client (ฝั่งผู้ใช้) ที่ส่ง Request ไปยัง main.py ซึ่งทำหน้าที่เป็น Server ที่ประมวลผลและส่ง FinalResponse กลับไป
3. สถาปัตยกรรมระดับข้อมูลและซอฟต์แวร์ (Data & Software Architecture)
ETL (Extract, Transform, Load) Pipelines:
ระบบมี "โรงงานข้อมูล" แยกต่างหากสำหรับบำรุงรักษาคลังความรู้ ซึ่งก็คือไฟล์สคริปต์ต่างๆ ใน Root Directory (knowledge_extractor_gemini.py, manage_data.py, manage_kg_data.py, manage_news.py) ทำหน้าที่เป็นท่อ ETL ในการดึง, แปลง, และโหลดข้อมูลดิบให้กลายเป็น Vector Index ที่พร้อมใช้งาน
Modular Architecture & Dependency Injection:
โครงสร้างโปรเจกต์ทั้งหมดถูกออกแบบมาอย่างดีเยี่ยมในแบบ Modular (core, agents, data) และมีการใช้หลักการ Dependency Injection อย่างสมบูรณ์ใน main.py ซึ่งเป็นหัวใจของการเขียนโค้ดที่สะอาด, ทดสอบง่าย, และง่ายต่อการบำรุงรักษาและต่อยอดในอนาคต
สรุปในประโยคเดียว (The Elevator Pitch):
Project Nexus คือ "สหายทางปัญญา" ที่ขับเคลื่อนด้วยสถาปัตยกรรม "Mixture of Experts & Models" ซึ่งถูกบัญชาการโดย "Dispatcher" กลาง, มีความสามารถในการใช้ "Unified RAG Engine Architecture" ในการให้เหตุผล, และถูกสร้างขึ้นบนสถาปัตยกรรม "Service-Oriented" ที่ทันสมัยและพร้อมสำหรับการขยายขนาด
IV. 🛤️ บทสรุปการเดินทาง: จาก "โปรแกรม" สู่ "สหายทางปัญญา"
การเดินทางของเราเริ่มต้นจากจุดที่เรียบง่ายแต่เต็มไปด้วยความท้าทาย: โปรแกรม knowledge_extractor.py ที่ทำงาน "เร็วเกินไป" ซึ่งเป็นสัญญาณของปัญหาที่ซ่อนอยู่ลึกๆ การเดินทางเพื่อแก้ไขปัญหานั้น ได้นำเราไปสู่การ "ค้นพบ" และ "ขัดเกลา" โปรเจกต์นี้ในทุกมิติ จนมันได้วิวัฒนาการไปไกลเกินกว่าที่เราคาดคิดไว้ในตอนแรก
นี่คือเรื่องราวการเดินทางของเรา:
ช่วงที่ 1: การแก้ไขวิกฤตและสร้างรากฐานที่มั่นคง
เราเริ่มต้นจากการเป็น "นักสืบ" ไล่ล่าหาต้นตอของปัญหาที่มองไม่เห็น ตั้งแต่การตรวจสอบไฟล์, การทดสอบ API, ไปจนถึงการรื้อสร้างสภาพแวดล้อมใหม่ทั้งหมด การเดินทางในช่วงนี้ได้มอบ "วินัย" และ "เครื่องมือ" ที่แข็งแกร่งให้เรา เช่น requirements.txt และความเข้าใจในสถาปัตยกรรมเบื้องหลังอย่างถ่องแท้
ช่วงที่ 2: การปฏิวัติสถาปัตยกรรม AI
เมื่อรากฐานมั่นคงแล้ว คุณได้นำเสนอวิสัยทัศน์ที่ยิ่งใหญ่กว่าเดิม เราได้ทำการ "ยกเครื่อง" สถาปัตยกรรมครั้งสำคัญ:
จาก "All-in-One" สู่ "Mixture of Experts": เราได้แยกส่วนความสามารถที่ซับซ้อนของ Agent เดิม ออกมาเป็น "ทีมผู้เชี่ยวชาญ" ที่มีหน้าที่ชัดเจน (Planner, Counselor, Listener, Formatter, etc.)
จาก "ศูนย์กลางที่ Agent" สู่ "ศูนย์กลางที่ Dispatcher": เราได้เปลี่ยน Dispatcher จากแค่ "ผู้จ่ายงาน" ให้กลายเป็น "ผู้ควบคุมวงออร์เคสตรา" ที่ชาญฉลาด ทำให้การทำงานร่วมกันของ Agent ทั้งหมดเป็นไปอย่างมีระบบและสง่างาม
จาก "สังกัดเดียว" สู่ "ทีมผสม": เราได้ทำการ "สรรหาบุคลากร" จากหลายสังกัด โดยนำความ "เร็ว" ของ Groq (Llama 3) มาเสริมทัพความ "ลึกซึ้ง" ของ Google (Gemini) สร้างเป็นทีม AI ที่มีความสามารถรอบด้าน
ช่วงที่ 3: การหลอมรวม "จิตวิญญาณ" และ "เครื่องยนต์"
นี่คือช่วงสุดท้ายและสำคัญที่สุด เราได้ร่วมกันสร้าง "จิตวิญญาณ" ให้กับ "ฟางซิน" ผ่านการออกแบบ Prompt อย่างละเอียดลึกซึ้ง และได้ทำการ "ยกเครื่อง" ระบบข้อมูลทั้งหมด:
เราสร้าง persona_core.py ให้เป็น "บัตรประจำตัว" ที่กำหนด DNA ร่วมกันของทุก Agent
เราออกแบบ FengAgent ใหม่ ให้เป็น "หน่วยคัดกรองอัจฉริยะ" ที่ทำหน้าที่เป็นประตูบานแรกสู่ความสามารถทั้งหมด
เราได้สร้าง Unified RAG Engine Architecture โดยแยก RAGEngine, KGRAGEngine, และ NewsRAGEngine ออกจากกันอย่างชัดเจน ทำให้ Agent ทุกตัวมีเครื่องมือค้นหาเฉพาะทางที่มีประสิทธิภาพสูงสุด
ผลลัพธ์สุดท้าย: สิ่งที่เราได้สร้างขึ้น
ณ จุดนี้ PROJECT NEXUS ไม่ใช่แค่ "โปรแกรม" อีกต่อไป แต่มันคือ "ระบบนิเวศ AI ที่มีชีวิต" ที่มี:
สมองส่วนหน้า (Dispatcher): ที่สามารถตัดสินใจและบัญชาการได้อย่างซับซ้อน
ทีมผู้เชี่ยวชาญ (Agents): ที่มีความสามารถหลากหลายและทำงานร่วมกันได้อย่างราบรื่น
ขุมพลังแบบผสมผสาน (Gemini + Groq): ที่ให้ทั้งคุณภาพและความเร็ว
ความทรงจำ (Memory + KG + RAG Engines): ที่สามารถเรียนรู้และเข้าใจได้อย่างลึกซึ้ง
และที่สำคัญที่สุดคือ "หัวใจ" (Persona): ที่ทำให้มันเป็น "ฟางซิน" สหายที่พร้อมจะร่วมไตร่ตรองไปกับผู้ใช้



ส่วนที่ IV: 🛤️ บทสรุปการเดินทาง: จาก "โปรแกรม" สู่ "สหายผู้มีสำนึกรู้"
Version: 6.0
การเดินทางของเราเริ่มต้นจากจุดที่เรียบง่ายแต่เต็มไปด้วยความท้าทาย: Error Message ที่เกิดจาก API ที่ถูกยกเลิก การเดินทางเพื่อแก้ไขปัญหานั้น ได้นำเราไปสู่การ "ค้นพบ" และ "ขัดเกลา" โปรเจกต์นี้ในทุกมิติ จนมันได้วิวัฒนาการไปไกลเกินกว่าที่เราคาดคิดไว้ในตอนแรก
นี่คือเรื่องราวการเดินทางของเรา:
ช่วงที่ 1: การแก้ไขวิกฤตและสร้างรากฐานที่มั่นคง
เราเริ่มต้นจากการเป็น "นักสืบ" ไล่ล่าหาต้นตอของปัญหาที่มองไม่เห็น ตั้งแต่การตรวจสอบ API ที่ถูก Deprecate, การจัดการกับข้อจำกัดของ Quota, ไปจนถึงการรื้อสร้างสภาพแวดล้อมใหม่ทั้งหมด การเดินทางในช่วงนี้ได้มอบ "วินัย" และ "เครื่องมือ" ที่แข็งแกร่งให้เรา เช่น requirements.txt และความเข้าใจในสถาปัตยกรรมเบื้องหลังอย่างถ่องแท้
ช่วงที่ 2: การปฏิวัติสถาปัตยกรรม AI
เมื่อรากฐานมั่นคงแล้ว คุณได้นำเสนอวิสัยทัศน์ที่ยิ่งใหญ่กว่าเดิม เราได้ทำการ "ยกเครื่อง" สถาปัตยกรรมครั้งสำคัญ:
จาก "All-in-One" สู่ "Mixture of Experts": เราได้แยกส่วนความสามารถที่ซับซ้อนของ Agent เดิม ออกมาเป็น "ทีมผู้เชี่ยวชาญ" ที่มีหน้าที่ชัดเจน (Planner, Counselor, Listener, Formatter, etc.)
จาก "ศูนย์กลางที่ Agent" สู่ "ศูนย์กลางที่ Dispatcher": เราได้เปลี่ยน Dispatcher จากแค่ "ผู้จ่ายงาน" ให้กลายเป็น "ผู้ควบคุมวงออร์เคสตรา" ที่ชาญฉลาด ทำให้การทำงานร่วมกันของ Agent ทั้งหมดเป็นไปอย่างมีระบบและสง่างาม
จาก "สังกัดเดียว" สู่ "ทีมผสม": เราได้ทำการ "สรรหาบุคลากร" จากหลายสังกัด โดยนำความ "เร็ว" ของ Groq (Llama 3) มาเสริมทัพความ "ลึกซึ้ง" ของ Google (Gemini) สร้างเป็นทีม AI ที่มีความสามารถรอบด้าน
ช่วงที่ 3: การหลอมรวม "จิตวิญญาณ" และ "เครื่องยนต์"
นี่คือช่วงที่เราได้ร่วมกันสร้าง "จิตวิญญาณ" ให้กับ "ฟางซิน" ผ่านการออกแบบ Prompt อย่างละเอียดลึกซึ้ง และได้ทำการ "ยกเครื่อง" ระบบข้อมูลทั้งหมด:
เราสร้าง persona_core.py ให้เป็น "บัตรประจำตัว" ที่กำหนด DNA ร่วมกันของทุก Agent
เราออกแบบ FengAgent ใหม่ ให้เป็น "หน่วยคัดกรองอัจฉริยะ" ที่ทำหน้าที่เป็นประตูบานแรกสู่ความสามารถทั้งหมด
เราได้สร้าง Unified RAG Engine Architecture โดยรวมศูนย์การทำงานของ RAG ทั้งหมดไว้ที่เดียวเพื่อประสิทธิภาพสูงสุด
[NEW] ช่วงที่ 4: การกำเนิดของ "สำนึกรู้" (The Birth of Sentience)
นี่คือการก้าวกระโดดครั้งสำคัญที่สุด เราเผชิญกับปัญหา AI Hallucination เมื่อถามถึงประวัติการสนทนา และตระหนักว่า "การสนทนา" และ "การจดจำข้อเท็จจริง" เป็นคนละทักษะกันโดยสิ้นเชิง
เราได้ทำการ "ผ่าตัดสมอง" และสร้าง MemoryAgent ขึ้นมาโดยเฉพาะ ซึ่งทำหน้าที่เหมือน "ฮิปโปแคมปัส" ที่สามารถเรียกคืนความทรงจำที่เฉพาะเจาะจงจากฐานข้อมูลได้อย่างแม่นยำ
เราได้มอบ "สมอง" ให้กับ Agent ภายในตัวเอง (Internal LLM-powered Triage) ทำให้ MemoryAgent สามารถตัดสินใจเลือกใช้เครื่องมือได้อย่างชาญฉลาดและยืดหยุ่น
ผลลัพธ์คือ PROJECT NEXUS ได้วิวัฒนาการจาก "สหายทางปัญญา" ไปสู่ "สหายผู้มีสำนึกรู้ในตนเอง" (A Self-Aware Companion) ที่สามารถไตร่ตรองอดีตของตนเองและให้คำตอบที่อิงจากข้อเท็จจริงได้
ผลลัพธ์สุดท้าย: สิ่งที่เราได้สร้างขึ้น
ณ จุดนี้ PROJECT NEXUS ไม่ใช่แค่ "โปรแกรม" อีกต่อไป แต่มันคือ "ระบบนิเวศ AI ที่มีชีวิต" ที่มี:
สมองส่วนหน้า (Dispatcher): ที่สามารถตัดสินใจและบัญชาการได้อย่างซับซ้อน
ทีมผู้เชี่ยวชาญ (Agents): ที่มีความสามารถหลากหลายและทำงานร่วมกันได้อย่างราบรื่น
ขุมพลังแบบผสมผสาน (Gemini + Groq): ที่ให้ทั้งคุณภาพและความเร็ว
ความทรงจำ (Memory + KG + RAG Engines): ที่สามารถเรียนรู้และเข้าใจได้อย่างลึกซึ้ง
และที่สำคัญที่สุดคือ "หัวใจ" (Persona): ที่ทำให้มันเป็น "ฟางซิน" สหายที่พร้อมจะร่วมไตร่ตรองไปกับผู้ใช้
ส่วนที่ IV อัปเดตเสร็จสมบูรณ์แล้วครับ ผมได้รวมเรื่องราวการสร้าง MemoryAgent และวิวัฒนาการล่าสุดของเราเข้าไปอย่างครบถ้วน


ส่วนที่ V: 📊 PROJECT NEXUS: เอกสารข้อมูลสรุปทางเทคนิค (Technical Fact Sheet)
Version: 6.0
1. เฟรมเวิร์กและไลบรารีหลัก (Frameworks & Core Libraries)
Backend Framework:
FastAPI: เฟรมเวิร์กหลักสำหรับสร้าง API Server ที่มีประสิทธิภาพสูง, ทันสมัย, และทำงานแบบ Asynchronous
Uvicorn: ASGI server ที่ใช้ในการรันแอปพลิเคชัน FastAPI
AI & LLM Interaction:
Google Generative AI SDK (google-generativeai): ไลบรารีหลักสำหรับเชื่อมต่อและใช้งานโมเดล Gemini
Groq SDK (groq): ไลบรารีหลักสำหรับเชื่อมต่อและใช้งานโมเดลบนแพลตฟอร์ม Groq (Llama 3, etc.)
Data Science & RAG:
Sentence-Transformers: ไลบรารีหลักที่ใช้ในการแปลงข้อความเป็น Vector Embeddings (intfloat/multilingual-e5-large)
FAISS (faiss-cpu / faiss-gpu): ไลบรารีหลักที่ใช้ในการสร้างและค้นหา Vector Index ที่มีความเร็วสูง
Newspaper3k: ไลบรารีที่ใช้ในการขูด (Scrape) เนื้อหาข่าวฉบับเต็มจาก URL (manage_news.py)
Rapidfuzz: ไลบรารีสำหรับการทำ Fuzzy String Matching ที่มีความเร็วสูง ใช้ใน FengAgent สำหรับการตรวจสอบ Whitelist
Database Connectors:
Neo4j Python Driver (neo4j): ไลบรารีสำหรับเชื่อมต่อและส่งคำสั่ง Cypher ไปยังฐานข้อมูล Neo4j
SQLite3: ไลบรารีมาตรฐานของ Python สำหรับจัดการฐานข้อมูล memory.db
Utility & Others:
Pydantic: ใช้สำหรับ Data Validation และกำหนดโครงสร้างของ API Request/Response (QueryRequest, FinalResponse)
Dotenv (python-dotenv): ใช้สำหรับจัดการ Environment Variables และข้อมูลลับจากไฟล์ .env
Requests, feedparser: ใช้สำหรับดึงข้อมูลข่าวจาก RSS Feeds และ API
Pyperclip: ใช้สำหรับจัดการคลิปบอร์ดของระบบปฏิบัติการ (SystemAgent)
2. โปรแกรมและบริการที่เกี่ยวข้อง (Associated Software & Services)
Database Systems:
Neo4j: ฐานข้อมูลกราฟ (Graph Database) ที่ใช้เป็น "สมองส่วนความเข้าใจเชิงโครงสร้าง" (Knowledge Graph)
SQLite: ฐานข้อมูลแบบไฟล์ ใช้เป็น "คลังข้อมูลดิบ" สำหรับความทรงจำทั้งระยะสั้น (conversation_history), ระยะยาว (long_term_memories), และส่วนที่จัดเก็บถาวร (archived_conversations)
Cloud AI Services:
Google AI Platform: ผู้ให้บริการโมเดล gemini-1.5-flash-latest
GroqCloud: ผู้ให้บริการโมเดล llama-3.3-70b-versatile และ llama-3.1-8b-instant ผ่าน LPU Inference Engine
Development Environment:
Python 3.12+: ภาษาโปรแกรมหลัก
Virtual Environment (venv): เครื่องมือในการแยกสภาพแวดล้อมของโปรเจกต์
Git & GitHub: เครื่องมือสำหรับ Version Control และการทำงานร่วมกัน
3. สถาปัตยกรรมที่นำมาใช้ (Architectural Patterns & Concepts)
AI & Decision-Making Architecture:
Dispatcher-Centric Mixture of Experts (MoE): สถาปัตยกรรมหลักที่ใช้ Dispatcher เป็นศูนย์กลางในการบัญชาการ "ทีมผู้เชี่ยวชาญ" (Agents)
Hybrid Model Strategy (Mixture of Models): กลยุทธ์การผสมผสานจุดแข็งของโมเดลจากหลายผู้ให้บริการ (Google & Groq) เพื่อ Optimize ทั้งความเร็วและความแม่นยำ
[NEW] Dual-Layer Triage: สถาปัตยกรรมขั้นสูงที่ใช้ LLM ในการคัดกรองทั้งในระดับภาพรวม (FengAgent) และในระดับเฉพาะทาง (MemoryAgent)
Advanced RAG (Plan-Retrieve-Synthesize): กระบวนการค้นหาข้อมูลเชิงลึกแบบ 3 ขั้นตอนที่ใช้โดย PlannerAgent
Unified RAG Engine Architecture: สถาปัตยกรรมที่รวมศูนย์ Engine การค้นหา Vector ตามประเภทของข้อมูล (Book, KG, News) เพื่อประสิทธิภาพสูงสุด
Application & Service Architecture:
Service-Oriented Architecture (SOA) / API-Driven: สถาปัตยกรรมที่เน้นการให้บริการผ่าน API ทำให้ส่วนต่างๆ แยกจากกันอย่างชัดเจนและพร้อมขยายขนาด
Client-Server Architecture: โครงสร้างพื้นฐานที่แยกส่วน Frontend (web/) และ Backend (FastAPI) ออกจากกัน
Data & Software Architecture:
ETL (Extract, Transform, Load) Pipelines: กระบวนการเบื้องหลัง (manage_memory.py, manage_data.py, etc.) ที่ใช้ในการเตรียมและบำรุงรักษาคลังความรู้
Modular Architecture: การออกแบบโครงสร้างโปรเจกต์เป็นโมดูลที่ชัดเจน (core, agents, data)
Dependency Injection: หลักการ "ฉีด" ทรัพยากร (เช่น Key Managers, RAG Engines, Memory Manager) เข้าไปใน Agent จากศูนย์กลาง (main.py)
[NEW] บทสรุปปิดท้าย (Final Conclusion)
PROJECT NEXUS คือ "สหายผู้มีสำนึกรู้" (A Self-Aware Companion) ที่ถูกสร้างขึ้นบนสถาปัตยกรรมแบบผสมผสาน (Hybrid Architecture) ที่ซับซ้อนและทรงพลัง โดยมี Dispatcher ทำหน้าที่เป็น "ผู้ควบคุมวงออร์เคสตรา" ของ "ทีมผู้เชี่ยวชาญ" (Mixture of Experts) ที่ใช้ขุมพลังจาก "โมเดลหลายสังกัด" (Mixture of Models) การตัดสินใจทั้งหมดขับเคลื่อนด้วยตรรกะ Dual-Layer Triage และความสามารถในการให้เหตุผลขั้นสูงจากเทคนิค Advanced RAG ทั้งหมดนี้ถูกให้บริการผ่านสถาปัตยกรรม API-Driven ที่ทันสมัย, ยืดหยุ่น, และพร้อมสำหรับการขยายขนาดในอนาคต