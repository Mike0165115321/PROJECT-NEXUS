# agents/feng_mode/general_conversation_agent.py
import json
from typing import Dict, List, Any
from groq import AsyncGroq  
import asyncio

class GeneralConversationAgent:
    
    def __init__(self, key_manager, model_name: str, rag_engine, ltm_manager, persona_prompt: str):
        self.key_manager = key_manager
        self.model_name = model_name
        self.rag_engine = rag_engine
        self.ltm_manager = ltm_manager
        
        self.general_conversation_prompt = persona_prompt + """
**‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à: ‡∏™‡∏´‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ú‡∏π‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥**

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö:**
- **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ):**
{long_term_memory_context}
- **‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î:**
{history_context}
- **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏™‡∏±‡∏ç‡∏ä‡∏≤‡∏ï‡∏ç‡∏≤‡∏ì (KG-RAG):**
{intuitive_context}

**‡∏®‡∏¥‡∏•‡∏õ‡∏∞‡πÅ‡∏´‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:**
1.  **‡πÑ‡∏ï‡∏£‡πà‡∏ï‡∏£‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥:** ‡∏≠‡πà‡∏≤‡∏ô "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß" ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï
2.  **‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏≤‡∏ç‡∏â‡∏•‡∏≤‡∏î:** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:** "{query}"
**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏ü‡∏≤‡∏á‡∏ã‡∏¥‡∏ô (‡∏ó‡∏µ‡πà‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î):**
"""
        print("ü§ù General Conversation Agent (V6 - KGRAG Powered) is ready.")
    
    async def _get_intuitive_context(self, query: str) -> str:
        print(f" 	- üï∏Ô∏è  Searching KG-RAG (Async) for intuition about: '{query}'")
        if not self.rag_engine:
            return "‡πÑ‡∏°‡πà‡∏°‡∏µ"
        
        results = await self.rag_engine.search_graph(query, top_k=2)
        
        if not results:
            return "‡πÑ‡∏°‡πà‡∏°‡∏µ"
            
        contexts = [f"- '{item.get('name')}': {item.get('description', '')[:70]}..." for item in results]
        return "\n".join(contexts)

    async def handle(self, query: str, short_term_memory: List[Dict[str, Any]]) -> str:
        print(f"üí¨ [General Conversation Agent V12] Handling: '{query[:40]}...' (Async)")
        ltm_context = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
        if self.ltm_manager:
            try:
                relevant_memories = await asyncio.to_thread(
                    self.ltm_manager.search_relevant_memories, query, k=2
                )
                if relevant_memories:
                    ltm_context = "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\n"
                    ltm_context += "\n\n".join([
                        f"- ‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ '{mem.get('title')}':\n  {mem.get('summary')}"
                        for mem in relevant_memories
                    ])
            except Exception as ltm_e:
                print(f"‚ùå GeneralConversationAgent LTM Error: {ltm_e}")
                ltm_context = "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß"
        
        api_key = await self.key_manager.get_key()
        if not api_key:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ"

        try:
            client = AsyncGroq(api_key=api_key)
            
            intuitive_context = await self._get_intuitive_context(query)
            
            history_context = "\n".join([f"- {mem.get('role')}: {mem.get('content')}" for mem in short_term_memory])
            
            prompt = self.general_conversation_prompt.format(
                long_term_memory_context=ltm_context,
                intuitive_context=intuitive_context,
                history_context=history_context,
                query=query
            )
            
            chat_completion = await client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model_name,
            )
            return chat_completion.choices[0].message.content.strip()
        
        except Exception as e:
            print(f"‚ùå GeneralConversationAgent LLM Error: {e}")
            if api_key: self.key_manager.report_failure(api_key)
            
            if api_key and ("429" in str(e).lower() or "service_unavailable" in str(e).lower()):
                print(" 	 -> Retrying with a new key...")
                await asyncio.sleep(1)
                return await self.handle(query, short_term_memory)

            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"