# knowledge_extractor_gemini.py
# (V13 - Safe & Production-Ready)
# ‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡∏™‡∏π‡πà‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î

import json
import os
import time
import re
import traceback
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional
from google.api_core.exceptions import ResourceExhausted, TooManyRequests, GoogleAPICallError
import google.generativeai as genai
from neo4j import GraphDatabase
from tqdm import tqdm

from core.config import settings
from core.api_key_manager import ApiKeyManager, AllKeysOnCooldownError

class KnowledgeGraphExtractorGemini:
    """
    ‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "Gemini"
    (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÉ‡∏ä‡πâ Batch Write ‡πÅ‡∏•‡∏∞ File Safety Standard)
    """
    def __init__(self, key_manager: ApiKeyManager, model_name: str, neo4j_driver):
        self.key_manager = key_manager
        self.neo4j_driver = neo4j_driver
        self.model_name = model_name
        self.extraction_prompt_template = """
**‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:**  
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "‡∏™‡∏ñ‡∏≤‡∏õ‡∏ô‡∏¥‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" (Data Architect) ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå JSON ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô "‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß Knowledge Graph" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

**‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**
- ‡∏™‡∏£‡πâ‡∏≤‡∏á "‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á" (Nodes, Edges, Labels) ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á
- ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "description" ‡πÅ‡∏•‡∏∞ "insight" ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö ‡∏ó‡∏µ‡∏°‡∏°‡∏±‡∏ì‡∏ë‡∏ô‡∏≤‡∏Å‡∏£ (Refinement Team) ‡∏à‡∏∞‡∏°‡∏≤‡∏Ç‡∏±‡∏î‡πÄ‡∏Å‡∏•‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á

---
## ‚õìÔ∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ (INPUT SCHEMA):
```json
{{
  "book_title": "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠",
  "category": "‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠",
  "chapter_title": "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ó",
  "subsection_title": "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢",
  "title": "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ",
  "description": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ",
  "content": "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ï‡πá‡∏°‡πÜ"
}}
üß† ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (OUTPUT SCHEMA) (‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô):
json
{{
  "nodes": [
    {{ "id": "Unique ID ‡∏Ç‡∏≠‡∏á‡πÇ‡∏´‡∏ô‡∏î", "label": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡πÇ‡∏´‡∏ô‡∏î (Node Label)", "properties": {{...}} }}
  ],
  "edges": [
    {{ "source": "ID ‡πÇ‡∏´‡∏ô‡∏î‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á", "target": "ID ‡πÇ‡∏´‡∏ô‡∏î‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á", "label": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Relationship Type)" }}
  ]
}}
üìê ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô:
Book Node: ‡∏™‡∏£‡πâ‡∏≤‡∏á ID ‡∏î‡πâ‡∏ß‡∏¢ Book:{{book_title}}. Label ‡∏Ñ‡∏∑‡∏≠ Book. Property ‡∏Ñ‡∏∑‡∏≠ {{ "name": "{{book_title}}", "category": "{{category}}" }}
Chapter Node: (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏™‡∏£‡πâ‡∏≤‡∏á ID ‡∏î‡πâ‡∏ß‡∏¢ Chapter:{{book_title}}-{{chapter_title}}. Label ‡∏Ñ‡∏∑‡∏≠ Chapter. Property ‡∏Ñ‡∏∑‡∏≠ {{ "name": "{{chapter_title}}" }}. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° (Book) -[:HAS_CHAPTER]-> (Chapter)
Subsection Node: (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏™‡∏£‡πâ‡∏≤‡∏á ID ‡∏î‡πâ‡∏ß‡∏¢ Subsection:{{book_title}}-{{chapter_title}}-{{subsection_title}}. Label ‡∏Ñ‡∏∑‡∏≠ Subsection. Property ‡∏Ñ‡∏∑‡∏≠ {{ "name": "{{subsection_title}}" }}. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° (Chapter) -[:HAS_SUBSECTION]-> (Subsection)
‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏´‡∏ô‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Main Concept Node):
‡πÉ‡∏ä‡πâ title ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á‡πÇ‡∏´‡∏ô‡∏î‡∏´‡∏•‡∏±‡∏Å ‡∏™‡∏£‡πâ‡∏≤‡∏á ID ‡∏î‡πâ‡∏ß‡∏¢ Concept:{{title}}. Label ‡∏Ñ‡∏∑‡∏≠ Concept (‡∏´‡∏£‡∏∑‡∏≠ Strategy, Technique ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)
‡∏™‡∏£‡πâ‡∏≤‡∏á property description ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå description ‡πÅ‡∏•‡∏∞ content ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö
‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏´‡∏ô‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏´‡∏ô‡∏î‡∏´‡∏•‡∏±‡∏Å: (Subsection ‡∏´‡∏£‡∏∑‡∏≠ Chapter) -[:MENTIONS]-> (Concept)
‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à (Enrichment):
‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° properties: strategy_type, influence_level, adaptability_level ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Concept Node
Insight Property: ‡∏™‡∏Å‡∏±‡∏î "‡πÅ‡∏Å‡πà‡∏ô‡πÅ‡∏ó‡πâ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á" ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô property ‡∏ä‡∏∑‡πà‡∏≠ insight
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏´‡∏ô‡∏î‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (Lists):
Psychological Technique Nodes: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô psychological_techniques ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á Node ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ID Technique:{{‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ}} ‡πÅ‡∏•‡∏∞ Label Technique. ‡πÄ‡∏û‡∏¥‡πà‡∏° Property {{ "name": "{{‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ}}", "type": "Psychological" }}
Risk Factor Nodes: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô risk_factors ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á Node ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ID Risk:{{‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á}} ‡πÅ‡∏•‡∏∞ Label Risk. ‡πÄ‡∏û‡∏¥‡πà‡∏° Property {{ "name": "{{‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á}}" }}
Control Technique Nodes: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô control_techniques ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á Node ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ID Technique:{{‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°}} ‡πÅ‡∏•‡∏∞ Label Technique. ‡πÄ‡∏û‡∏¥‡πà‡∏° Property {{ "name": "{{‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°}}", "type": "Control" }}
‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å (Deep Relationships):
(Main Concept) -[:USES_TECHNIQUE]-> (Technique)
(Main Concept) -[:HAS_RISK]-> (Risk)
(Risk) -[:MITIGATED_BY]-> (Control Technique)
üéØ ‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö:
‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏£‡∏∞‡∏î‡∏π‡∏Å" ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏°‡∏≤ "‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏¥‡∏ï‡∏ß‡∏¥‡∏ç‡∏ç‡∏≤‡∏ì" ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á

JSON CHUNK ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:
json
{text_chunk}
‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON:
"""

    def _extract_json(self, text: str) -> Optional[Dict]:
        text = text.strip()
        match = re.search(r'```(json)?\s*(\{[\s\S]*?\})\s*```', text, re.DOTALL)
        if match:
            json_str = match.group(2)
            try: return json.loads(json_str)
            except json.JSONDecodeError: pass
        brace_level, start_index = 0, -1
        for i, char in enumerate(text):
            if char == '{':
                if start_index == -1: start_index = i
                brace_level += 1
            elif char == '}':
                if start_index != -1:
                    brace_level -= 1
                    if brace_level == 0:
                        json_str = text[start_index : i + 1]
                        try: return json.loads(json_str)
                        except json.JSONDecodeError: return None
        return None

    def _process_single_chunk(self, line: str, max_retries: int) -> Optional[Dict]:
        if not line.strip(): return None
        try:
            item = json.loads(line)
            text_to_process = json.dumps(item, ensure_ascii=False, indent=2)
            prompt = self.extraction_prompt_template.format(text_chunk=text_to_process)
        except json.JSONDecodeError:
            print(f"Skipping malformed JSON line: {line[:100]}...")
            return None
        last_exception = None
        for attempt in range(max_retries):
            try:
                api_key = self.key_manager.get_key()
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel(self.model_name)
                safety_settings = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                ]
                response = model.generate_content(prompt, safety_settings=safety_settings)
                if response.prompt_feedback and response.prompt_feedback.block_reason:
                    raise ValueError(f"Gemini API blocked the prompt. Reason: {response.prompt_feedback.block_reason}")
                graph_data = self._extract_json(response.text)
                if graph_data and isinstance(graph_data, dict) and "nodes" in graph_data:
                    for node in graph_data.get("nodes", []):
                        if node.get("label") in ["Concept", "Strategy"]:
                            if "properties" not in node: node["properties"] = {}
                            node["properties"]["raw_content"] = item.get("content", "")
                    return graph_data
                else:
                    raise ValueError(f"Failed to extract valid JSON. Raw response: {response.text[:200]}...")
            except (ResourceExhausted, TooManyRequests) as e:
                error_type = 'quota' if isinstance(e, ResourceExhausted) else 'rate_limit'
                print(f"üîª Key ...{api_key[-4:]} failed ({error_type}). Trying next key... ({attempt + 1}/{max_retries})")
                self.key_manager.report_failure(api_key, error_type=error_type)
                last_exception = e
            except (GoogleAPICallError, ValueError) as e:
                print(f"üí• Key ...{api_key[-4:]} encountered an error: {e}. Trying next key... ({attempt + 1}/{max_retries})")
                self.key_manager.report_failure(api_key, error_type='generic')
                last_exception = e
            except AllKeysOnCooldownError as e:
                raise e
        raise last_exception if last_exception else Exception("Failed to process chunk after all retries.")

    def _write_batch_to_neo4j(self, graph_data_list: List[Dict]):
        if not graph_data_list: return
        all_nodes, all_edges = [], []
        for graph_data in graph_data_list:
            nodes, edges = graph_data.get("nodes", []), graph_data.get("edges", [])
            for node in nodes:
                if node.get('id'):
                    node['id'] = str(node['id']).strip().lower()
                    all_nodes.append(node)
            for edge in edges:
                if edge.get('source') and edge.get('target'):
                    edge['source'] = str(edge['source']).strip().lower()
                    edge['target'] = str(edge['target']).strip().lower()
                    all_edges.append(edge)
        with self.neo4j_driver.session() as session:
            session.execute_write(self._create_nodes_and_edges_in_batch, all_nodes, all_edges)

    @staticmethod
    def _create_nodes_and_edges_in_batch(tx, nodes, edges):
        nodes_by_label = {}
        for node in nodes:
            label = node.get('label')
            if not label: continue
            cleaned_label = re.sub(r'[^a-zA-Z‡∏Å-‡πô]', '', str(label)) or "Concept"
            if cleaned_label not in nodes_by_label:
                nodes_by_label[cleaned_label] = []
            properties = node.get('properties', {})
            if 'name' not in properties and node.get('id'):
                properties['name'] = node['id'].split(':')[-1].replace('-', ' ').replace('_', ' ').title()
            nodes_by_label[cleaned_label].append({'id': node['id'], 'properties': properties})
        for label, node_list in nodes_by_label.items():
            query = (f"""
            UNWIND $node_data AS data
            MERGE (n:{label} {{id: data.id}})
            ON CREATE SET n = data.properties, n.id = data.id, n.created_at = timestamp(), n.updated_at = timestamp()
            ON MATCH SET n += data.properties, n.updated_at = timestamp()
            """)
            tx.run(query, node_data=node_list)
        edges_by_label = {}
        for edge in edges:
            label = edge.get('label')
            if not label: continue
            cleaned_label = re.sub(r'[^a-zA-Z_]', '', str(label)).upper()
            if not cleaned_label: continue
            if cleaned_label not in edges_by_label:
                edges_by_label[cleaned_label] = []
            edges_by_label[cleaned_label].append({'source': edge['source'], 'target': edge['target']})
        for label, edge_list in edges_by_label.items():
            query = (f"""
            UNWIND $edge_data AS data
            MATCH (a {{id: data.source}})
            MATCH (b {{id: data.target}})
            MERGE (a)-[r:{label}]->(b)
            """)
            tx.run(query, edge_data=edge_list)

    def _process_batch_parallel(self, lines_batch: List[str], max_workers: int) -> (List[Dict], List[str]):
        failed_lines, successful_graphs = [], []
        max_retries = len(self.key_manager.all_keys)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_line = {executor.submit(self._process_single_chunk, line, max_retries): line for line in lines_batch}
            progress_bar = tqdm(as_completed(future_to_line), total=len(lines_batch), desc="  - Processing Batch", leave=False, unit="chunk")
            for future in progress_bar:
                original_line = future_to_line[future]
                try:
                    graph_data = future.result()
                    if graph_data:
                        successful_graphs.append(graph_data)
                except AllKeysOnCooldownError as e:
                    print(f"\n‚ùå CRITICAL: {e}. Stopping all processing.")
                    for f in future_to_line: f.cancel()
                    raise e
                except Exception as e:
                    print(f"\nDEBUG: Line failed after all retries: {type(e).__name__} - {e}\n")
                    failed_lines.append(original_line)
                    progress_bar.set_postfix_str(f"fails={len(failed_lines)}", refresh=True)
        return successful_graphs, failed_lines

    def process_file_resiliently(self, file_path: str, batch_size: int, max_workers: int):
        file_name = os.path.basename(file_path)
        processed_folder = os.path.join(os.path.dirname(file_path), "_processed")
        os.makedirs(processed_folder, exist_ok=True)
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                all_lines = f.readlines()
            if not all_lines:
                print(f"  - ‚úÖ File '{file_name}' is empty. Moving to processed.")
                shutil.move(file_path, os.path.join(processed_folder, file_name))
                return
            print(f"\n--- üß† Processing: {file_name} (Model: {self.model_name}, Total: {len(all_lines)} lines) ---")
            lines_iterator = iter(all_lines)
            all_failed_lines = []
            with tqdm(total=len(all_lines), desc=f"Overall ({file_name})", unit="lines") as pbar:
                while True:
                    lines_batch = [line for _, line in zip(range(batch_size), lines_iterator)]
                    if not lines_batch:
                        break
                    successful_graphs, failed_lines_in_batch = self._process_batch_parallel(lines_batch, max_workers)
                    if successful_graphs:
                        self._write_batch_to_neo4j(successful_graphs)
                    if failed_lines_in_batch:
                        all_failed_lines.extend(failed_lines_in_batch)
                    pbar.update(len(lines_batch))
            if all_failed_lines:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.writelines(all_failed_lines)
                print(f"  - ‚ö†Ô∏è File state partially updated. {len(all_failed_lines)} failed lines remain in '{file_name}'.")
            else:
                shutil.move(file_path, os.path.join(processed_folder, file_name))
                print(f"  - ‚ú® File state fully updated. Moved '{file_name}' to processed folder.")
        except AllKeysOnCooldownError as e:
            raise e
        except Exception as e:
            print(f"‚ùå An unexpected error occurred while processing {file_path}: {e}")
            traceback.print_exc()

def main():
    start_time = time.time()
    print("--- üè≠ Starting Knowledge Graph Extraction Process (V13 - Gemini Production) ---")

    BATCH_SIZE = 50
    MAX_WORKERS = 2
    BOOKS_FOLDER = "data/books"
    MODEL_TO_USE = "gemini-1.5-flash-latest"
    
    from core.api_key_manager import ApiKeyManager, AllKeysOnCooldownError

    key_manager = ApiKeyManager(all_google_keys=settings.GOOGLE_API_KEYS, silent=True)
    print(f"üîë Using Google Key Manager for model: {MODEL_TO_USE}")

    neo4j_driver = None
    try:
        neo4j_driver = GraphDatabase.driver(
            settings.NEO4J_URI, auth=(settings.NEO4J_USER, settings.NEO4J_PASSWORD)
        )
        neo4j_driver.verify_connectivity()
        print("‚úÖ Successfully connected to Neo4j database.")

        extractor_instance = KnowledgeGraphExtractorGemini(
            key_manager=key_manager, model_name=MODEL_TO_USE, neo4j_driver=neo4j_driver
        )
        
        if not os.path.exists(BOOKS_FOLDER):
            print(f"‚ö†Ô∏è Books folder not found: {BOOKS_FOLDER}")
            return

        files_to_process = sorted([f for f in os.listdir(BOOKS_FOLDER) if f.endswith(".jsonl")])
        if not files_to_process:
            print(f"üü° No .jsonl files found in '{BOOKS_FOLDER}'.")
        else:
            print(f"üìö Found {len(files_to_process)} files to process.")
            for filename in files_to_process:
                book_file_path = os.path.join(BOOKS_FOLDER, filename)
                extractor_instance.process_file_resiliently(
                    file_path=book_file_path, 
                    batch_size=BATCH_SIZE, 
                    max_workers=MAX_WORKERS
                )

    except AllKeysOnCooldownError as e:
        print(f"\nüî•üî•üî• API KEYS EXHAUSTED - CRITICAL FAILURE üî•üî•üî•")
        print(f"   -> Reason: {e}")
        print(f"   -> Halting the entire extraction process. Remaining files will be processed in the next run.")
    except Exception as e:
        print(f"‚ùå A critical error occurred in the main process: {e}")
        traceback.print_exc()
    finally:
        if neo4j_driver:
            neo4j_driver.close()
            print("\nüîó Neo4j connection closed.")

        end_time = time.time()
        print(f"\n‚úÖ Knowledge Graph extraction process finished. Total time: {end_time - start_time:.2f} seconds.")


if __name__ == "__main__":
    main()