# (V6.1 - BGE-M3 Optimized, FP16 VRAM, Class Architecture, Dynamic Batching)
# ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß, ‡∏Ç‡∏π‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°, ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS Index + Mapping file (‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Class)

import feedparser
import requests
import faiss
import json
import os
import time
import torch
import datetime
from tqdm import tqdm
from newspaper import Article, Config, ArticleException
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Set
from concurrent.futures import ThreadPoolExecutor, as_completed 
from core.config import settings
from urllib.parse import urlparse
import traceback
import numpy as np 

class NewsBuilder:
    
    def __init__(self, model_name="BAAI/bge-m3"):
        print("‚öôÔ∏è  News Builder is initializing...")
        
        self.NEWS_INDEX_DIR = "data/news_index"
        self.NEWS_FAISS_PATH = os.path.join(self.NEWS_INDEX_DIR, "news_faiss.index")
        self.NEWS_MAPPING_PATH = os.path.join(self.NEWS_INDEX_DIR, "news_mapping.json")

        self.NEWS_API_URL = "https://newsapi.org/v2/top-headlines"
        self.RSS_FEEDS = {
            "Reuters Tech": "https://www.reuters.com/pf/api/v2/content/corp/rss/US/technology-news-idUSKBN0P204J20150622",
            "TechCrunch": "https://techcrunch.com/feed/",
            "Wired Top Stories": "https://www.wired.com/feed/rss",
            "Ars Technica": "http://feeds.arstechnica.com/arstechnica/index/",
            "The Verge": "https://www.theverge.com/rss/index.xml",
            "MIT Technology Review": "https://www.technologyreview.com/feed/",
            "Hacker News": "https://news.ycombinator.com/rss",
            "Scientific American": "http://rss.sciam.com/sciam/news",
            "ScienceDaily": "https://www.sciencedaily.com/rss/top.xml",
            
            "Reuters Business": "https://www.reuters.com/pf/api/v2/content/corp/rss/US/business-news-idUSKBN0P002020150615",
            "Bloomberg Markets": "https://feeds.bloomberg.com/markets/news.rss",
            "The Economist": "https://www.economist.com/finance-and-economics/rss.xml",
            "Harvard Business Review": "https://hbr.org/rss/topic/latest",
            "Financial Times": "https://www.ft.com/world?format=rss",
            "Wall Street Journal": "https://feeds.a.dj.com/rss/RSSWorldNews.xml",

            "BBC World": "http://feeds.bbci.co.uk/news/world/rss.xml",
            "Associated Press (AP)": "https://apnews.com/hub/ap-top-news/rss.xml",
            "The New York Times": "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
            "The Guardian": "https://www.theguardian.com/world/rss",
            "Al Jazeera English": "https://www.aljazeera.com/xml/rss/all.xml",
            
            "Google News (TH)": "https://news.google.com/rss?hl=th&gl=TH&ceid=TH:th", 
            "Thai PBS": "https://www.thaipbs.or.th/rss/news.xml",
            "Thairath": "https://www.thairath.co.th/rss/news.xml",
            "The Standard": "https://thestandard.co/feed/",
            "Blognone": "https://www.blognone.com/rss.xml",
            "Brand Buffet": "https://www.brandbuffet.in.th/feed/"
        }
        
        self.settings = settings
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"  - Initializing on device: {device.upper()}")
        
        self.model = SentenceTransformer(model_name, device="cpu")
        
        if device == "cuda":
            print("  - ‚ö°Ô∏è Converting model to FP16 (on CPU) for VRAM efficiency...")
            self.model.half()
            print("  - ‚ö°Ô∏è Moving FP16 model to CUDA...")
            self.model.to(device)
            
        print(f"‚úÖ Embedding model '{model_name}' loaded successfully (FP16: {device=='cuda'}).")


    def _sanitize_text(self, text: str) -> str:
        if not text:
            return ""
        text = text.replace("\u2028", " ").replace("\u2029", " ")
        text = " ".join(text.split()) 
        return text.strip()

    def fetch_from_newsapi(self) -> List[Dict]:
        print("üì∞ Fetching news from NewsAPI.org...")
        if not self.settings.NEWS_KEY:
            print("   - ‚ö†Ô∏è NewsAPI key not found in .env file.")
            return []
        
        params = {'country': 'us', 'pageSize': 20, 'apiKey': self.settings.NEWS_KEY}
        try:
            response = requests.get(self.NEWS_API_URL, params=params, timeout=15)
            response.raise_for_status()
            articles = response.json().get('articles', [])
            print(f"   - Fetched {len(articles)} articles from NewsAPI.")
            return [{
                "published_at": a.get("publishedAt"),
                "source_name": a.get("source", {}).get("name"),
                "title": a.get("title"),
                "description": a.get("description"),
                "url": a.get("url")
            } for a in articles]
        except Exception as e:
            print(f"   - ‚ùå NewsAPI Error: {e}")
            return []

    def fetch_from_rss(self, url: str, source: str) -> List[Dict]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á RSS Feed"""
        try:
            feed = feedparser.parse(url)
            return [{
                "published_at": entry.get("published", datetime.datetime.now().isoformat()),
                "source_name": source,
                "title": entry.get("title"),
                "description": entry.get("summary"),
                "url": entry.get("link")
            } for entry in feed.entries]
        except Exception as e:
            print(f"   - ‚ùå RSS Error ({source}): {e}")
            return []

    def scrape_article_content(self, url: str) -> str:
        try:
            config = Config()
            config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            config.request_timeout = 15

            article = Article(url, config=config)
            article.download()
            article.parse()

            return self._sanitize_text(article.text)
        except ArticleException:
            return ""
        except Exception:
            return ""
        
    def load_existing_urls(self) -> Set[str]:
        """‡πÇ‡∏´‡∏•‡∏î URL ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô Index"""
        if not os.path.exists(self.NEWS_MAPPING_PATH):
            return set()
        
        existing_urls = set()
        with open(self.NEWS_MAPPING_PATH, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                for item in data.values():
                    if url := item.get("url"):
                        existing_urls.add(url)
            except json.JSONDecodeError:
                print("   - ‚ö†Ô∏è Could not parse existing mapping file. Starting fresh.")
        
        print(f"üîç Found {len(existing_urls)} existing articles in the index.")
        return existing_urls

    def collect_and_scrape_articles(self, existing_urls: Set[str]) -> List[Dict]:
        print("--- üì∞ Starting News Collection (True Incremental Mode) ---")
        
        initial_articles = []
        with ThreadPoolExecutor(max_workers=15) as executor:
            print("  - Submitting fetch tasks (NewsAPI + RSS)...")
            futures = [executor.submit(self.fetch_from_newsapi)]
            for name, url in self.RSS_FEEDS.items():
                futures.append(executor.submit(self.fetch_from_rss, url, name))
            
            for future in as_completed(futures):
                try:
                    initial_articles.extend(future.result())
                except Exception as e:
                    print(f"   - ‚ùå A fetch task failed: {e}")

        new_articles_to_process = [
            article for article in initial_articles 
            if article.get("url") and article.get("url") not in existing_urls
        ]
        
        print(f"\nüî¨ Found {len(new_articles_to_process)} new articles to scrape.")
        if not new_articles_to_process:
            return []

        articles_by_domain = {}
        for article in new_articles_to_process:
            if url := article.get("url"):
                try:
                    domain = urlparse(url).netloc.replace('www.', '')
                    if domain not in articles_by_domain:
                        articles_by_domain[domain] = []
                    articles_by_domain[domain].append(article)
                except Exception:
                    continue

        full_articles = []
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_domain = {
                executor.submit(self.scrape_article_content, articles[0].get("url")): domain
                for domain, articles in articles_by_domain.items() if articles
            }
            
            progress_bar = tqdm(total=len(new_articles_to_process), desc="Scraping New Articles")

            while future_to_domain:
                for future in as_completed(future_to_domain):
                    domain = future_to_domain.pop(future)
                    try:
                        content = future.result()
                        article_data = articles_by_domain[domain].pop(0)
                        if content:
                            article_data['full_content'] = content
                            full_articles.append(article_data)
                    except Exception as e:
                        print(f"   - ‚ùå Scrape failed for a URL from {domain}: {e}")
                    
                    progress_bar.update(1)

                    if articles_by_domain[domain]:
                        next_article = articles_by_domain[domain][0]
                        new_future = executor.submit(self.scrape_article_content, next_article.get("url"))
                        future_to_domain[new_future] = domain
                    
                    time.sleep(0.2) # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏ô‡πÅ‡∏ö‡∏ô
            
            progress_bar.close()

        print(f"\nüíæ Collected {len(full_articles)} articles with full content.")
        return full_articles

    # --- [V6.1] MODIFIED FUNCTION: build_news_index ---
    # ‡∏•‡∏ö 'batch_size' parameter ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏ö‡∏ö‡πÑ‡∏î‡∏ô‡∏≤‡∏°‡∏¥‡∏Å
    def build_news_index(self, articles: List[Dict]):
        if not articles:
            print("üü° No new articles to build index.")
            return

        if os.path.exists(self.NEWS_FAISS_PATH):
            print("   - Appending to existing index...")
            index = faiss.read_index(self.NEWS_FAISS_PATH)
            with open(self.NEWS_MAPPING_PATH, "r", encoding="utf-8") as f:
                mapping = json.load(f)
        else:
            print("   - Creating new index...")
            index = None
            mapping = {}

        print(f"üß† Generating embeddings for {len(articles)} new articles...")
        
        # --- [V6.1] Step 1: ‡∏ß‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
        # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
        print("   - Step 1: Measuring and preparing all new articles...")
        jobs = []
        for article_data in articles:
            title = self._sanitize_text(article_data.get('title', ''))
            content = self._sanitize_text(article_data.get('full_content', ''))
            embedding_text = f"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {title}\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {content}"
            
            jobs.append({
                "article_data": article_data,
                "embedding_text": embedding_text,
                "length": len(embedding_text) # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏õ‡πá‡∏ô proxy
            })

        # --- [V6.1] Step 2: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏™‡∏±‡πâ‡∏ô‡πÑ‡∏õ‡∏¢‡∏≤‡∏ß ---
        print(f"   - Step 2: Sorting {len(jobs)} jobs by text length...")
        sorted_jobs = sorted(jobs, key=lambda x: x['length'])

        # --- [V6.1] Step 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Batch ‡πÅ‡∏ö‡∏ö‡πÑ‡∏î‡∏ô‡∏≤‡∏°‡∏¥‡∏Å ---
        print("   - Step 3: Encoding with Dynamic Batching...")
        
        # ‡∏¢‡πâ‡∏≤‡∏¢ tqdm ‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å loop
        pbar = tqdm(total=len(sorted_jobs), desc="   - Encoding Batches")
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î ID ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mapping (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
        start_id = len(mapping) 
        
        current_idx = 0
        while current_idx < len(sorted_jobs):
            
            # 3.1) ‡∏î‡∏∂‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô batch ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á (‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á list ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠)
            # ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏Ç‡∏ô‡∏≤‡∏î batch
            max_len_in_batch = sorted_jobs[current_idx]['length']

            # 3.2) ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏Ç‡∏ô‡∏≤‡∏î Batch (Dynamic Batch Size)
            # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£ (Heuristics) ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
            # ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏™‡∏∏‡∏î‡∏¢‡∏±‡∏á‡∏™‡∏±‡πâ‡∏ô -> ‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤
            # OOM ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏µ‡πà [4, 2048, 1024] (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 8192 "Token-like units")
            # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            
            # (‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏π‡∏ô‡πÑ‡∏î‡πâ ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á OOM ‡∏Å‡πá‡∏•‡∏î batch size ‡∏•‡∏á‡∏≠‡∏µ‡∏Å)
            if max_len_in_batch > 16000:   # ~4000+ tokens (‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å)
                dynamic_batch_size = 4     # ‡∏•‡∏î‡∏•‡∏á‡∏à‡∏≤‡∏Å 4 (‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ OOM) -> ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á 2 ‡∏´‡∏£‡∏∑‡∏≠ 1
            elif max_len_in_batch > 8000:  # ~2000+ tokens (‡∏¢‡∏≤‡∏ß)
                dynamic_batch_size = 8
            elif max_len_in_batch > 4000:  # ~1000+ tokens
                dynamic_batch_size = 32
            else:                          # < 1000 tokens (‡∏™‡∏±‡πâ‡∏ô)
                dynamic_batch_size = 64    # ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏î‡πâ
            
            # *** [V6.1] SAFETY CHECK (‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å V6.1) ***
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏û‡∏ö‡∏ß‡πà‡∏≤ batch_size = 4, len = 2048 (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 8000+ chars) ‡∏°‡∏±‡∏ô OOM
            # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ
            
            if max_len_in_batch > 8000:    # ~2000+ tokens (‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢ OOM)
                dynamic_batch_size = 2     # *** ‡πÉ‡∏ä‡πâ 2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ***
            elif max_len_in_batch > 4000:  # ~1000+ tokens
                dynamic_batch_size = 8
            elif max_len_in_batch > 2000:
                dynamic_batch_size = 32
            else:                          # < 500 tokens (‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å)
                dynamic_batch_size = 64    # ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏î‡πâ

            # 3.3) ‡∏™‡∏£‡πâ‡∏≤‡∏á Batch
            end_idx = min(current_idx + dynamic_batch_size, len(sorted_jobs))
            batch_jobs = sorted_jobs[current_idx:end_idx]
            
            if not batch_jobs:
                break # ‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡πÉ‡∏™‡πà‡πÑ‡∏ß‡πâ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß

            texts_to_embed = [job['embedding_text'] for job in batch_jobs]
            batch_articles_data = [job['article_data'] for job in batch_jobs]
            
            # 3.4) Encode (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
            new_embeddings = self.model.encode(
                texts_to_embed,
                show_progress_bar=False,
                convert_to_numpy=True
            ).astype("float32")
            
            faiss.normalize_L2(new_embeddings)

            # 3.5) Add to Index (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
            if index is None:
                print("   - (First batch) Initializing new index with IndexFlatIP.")
                index = faiss.IndexFlatIP(new_embeddings.shape[1])
            
            index.add(new_embeddings)

            # 3.6) Update Mapping (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)
            for j, article_data in enumerate(batch_articles_data):
                article_data['title'] = self._sanitize_text(article_data.get('title', ''))
                article_data['description'] = self._sanitize_text(article_data.get('description', ''))
                article_data['full_content'] = self._sanitize_text(article_data.get('full_content', ''))
                article_data['embedding_text'] = texts_to_embed[j]
                
                # ‡πÉ‡∏ä‡πâ 'start_id' ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ô‡∏±‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
                mapping[str(start_id + j)] = article_data

            # 3.7) ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Loop
            pbar.update(len(batch_jobs)) # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï progress bar ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á
            start_id += len(batch_jobs)  # ‡πÄ‡∏û‡∏¥‡πà‡∏° ID ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö batch ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            current_idx = end_idx        # ‡∏Ç‡∏¢‡∏±‡∏ö index ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á batch ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

        pbar.close() # ‡∏õ‡∏¥‡∏î Pbar ‡πÄ‡∏°‡∏∑‡πà‡∏≠ loop ‡∏à‡∏ö

        os.makedirs(self.NEWS_INDEX_DIR, exist_ok=True)
        faiss.write_index(index, self.NEWS_FAISS_PATH)
        with open(self.NEWS_MAPPING_PATH, "w", encoding="utf-8") as f:
            json.dump(mapping, f, ensure_ascii=False, indent=4)
        
        print(f"‚úÖ News RAG Index updated successfully! Total articles: {index.ntotal}")

if __name__ == "__main__":
    try:
        print("\n" + "="*60)
        print("--- üì∞ Starting News Intelligence Gathering & Indexing (V6.1) üì∞ ---")
        print("="*60)
        
        builder = NewsBuilder()
        
        existing_urls = builder.load_existing_urls()
        new_articles = builder.collect_and_scrape_articles(existing_urls)
        
        # ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á batch_size ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
        builder.build_news_index(new_articles)

    except KeyboardInterrupt:
        print("\n\nüõë Process interrupted by user (Ctrl+C).")
    except Exception as e:
        print(f"\n‚ùå A critical error occurred in the main process: {e}")
        traceback.print_exc()
    finally:
        print("\n" + "="*60)
        print("‚úÖ News RAG Index build process finished or was interrupted.")
        print("="*60)